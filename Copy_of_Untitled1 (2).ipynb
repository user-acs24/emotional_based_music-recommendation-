{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CukfocauT_hQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow import convert_to_tensor\n",
        "\n",
        "from keras.layers import Conv2D, Flatten, Dense, MaxPooling2D, BatchNormalization, ZeroPadding2D, Dropout\n",
        "from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "from keras.layers import MaxPooling2D\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "from keras.utils import plot_model\n",
        "import tensorflow_datasets as tfd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_IjCYUZLKco"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECgU_FEkK61J"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/Mood-Detection-Project/challenges-in-representation-learning-facial-expression-recognition-challenge/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gj7IMvTeLTly"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(path + 'icml_face_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SahCB0K5LVQH"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Akdd9NcTLZEG"
      },
      "outputs": [],
      "source": [
        "# filter rows with emotion == 1\n",
        "emotion_1 = data[data['emotion'] == 1]\n",
        "\n",
        "# repeat rows 5 times\n",
        "repeated_rows = pd.concat([emotion_1] * 6, ignore_index=True)\n",
        "\n",
        "#concating augmented data and making a new dataframe\n",
        "data2 = pd.concat([data, repeated_rows], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlE1UlPKLaze"
      },
      "outputs": [],
      "source": [
        "data2.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4YLdjxHLcme"
      },
      "outputs": [],
      "source": [
        "data2[data2['emotion'] == 1].count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o85sLrTELeH-"
      },
      "outputs": [],
      "source": [
        "data2.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHntcJIXLe8X"
      },
      "outputs": [],
      "source": [
        "def prepare_data(data):\n",
        "    \"\"\" Prepare data for modeling\n",
        "        input: data frame with labels und pixel data\n",
        "        output: image and label array \"\"\"\n",
        "\n",
        "    image_array = np.zeros(shape=(len(data), 48, 48))\n",
        "    image_label = np.array(list(map(int, data['emotion'])))\n",
        "\n",
        "    for i, row in enumerate(data.index):\n",
        "        image = np.fromstring(data.loc[row, ' pixels'], dtype=int, sep=' ')\n",
        "        image = np.reshape(image, (48, 48))\n",
        "        image_array[i] = image\n",
        "\n",
        "    return image_array, image_label\n",
        "\n",
        "\n",
        "def plot_examples(label=0):\n",
        "    fig, axs = plt.subplots(1, 5, figsize=(25, 12))\n",
        "    fig.subplots_adjust(hspace = .2, wspace=.2)\n",
        "    axs = axs.ravel()\n",
        "    for i in range(5):\n",
        "        idx = data[data['emotion']==label].index[i]\n",
        "        axs[i].imshow(train_images[idx][:,:,0], cmap='gray')\n",
        "        axs[i].set_title(emotions[train_labels[idx].argmax()])\n",
        "        axs[i].set_xticklabels([])\n",
        "        axs[i].set_yticklabels([])\n",
        "\n",
        "def plot_all_emotions():\n",
        "    fig, axs = plt.subplots(1, 7, figsize=(30, 12))\n",
        "    fig.subplots_adjust(hspace = .2, wspace=.2)\n",
        "    axs = axs.ravel()\n",
        "    for i in range(7):\n",
        "        idx = data[data['emotion']==i].index[i]\n",
        "        axs[i].imshow(train_images[idx][:,:,0], cmap='gray')\n",
        "        axs[i].set_title(emotions[train_labels[idx].argmax()])\n",
        "        axs[i].set_xticklabels([])\n",
        "        axs[i].set_yticklabels([])\n",
        "\n",
        "def plot_image_and_emotion(test_image_array, test_image_label, pred_test_labels, image_number):\n",
        "    \"\"\" Function to plot the image and compare the prediction results with the label \"\"\"\n",
        "\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12, 6), sharey=False)\n",
        "\n",
        "    bar_label = emotions.values()\n",
        "\n",
        "    axs[0].imshow(test_image_array[image_number], 'gray')\n",
        "    axs[0].set_title(emotions[test_image_label[image_number]])\n",
        "\n",
        "    axs[1].bar(bar_label, pred_test_labels[image_number], color='orange', alpha=0.7)\n",
        "    axs[1].grid()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def plot_compare_distributions(array1, array2, title1='', title2=''):\n",
        "    df_array1 = pd.DataFrame()\n",
        "    df_array2 = pd.DataFrame()\n",
        "    df_array1['emotion'] = array1.argmax(axis=1)\n",
        "    df_array2['emotion'] = array2.argmax(axis=1)\n",
        "\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12, 6), sharey=False)\n",
        "    x = emotions.values()\n",
        "\n",
        "    y = df_array1['emotion'].value_counts()\n",
        "    keys_missed = list(set(emotions.keys()).difference(set(y.keys())))\n",
        "    for key_missed in keys_missed:\n",
        "        y[key_missed] = 0\n",
        "    axs[0].bar(x, y.sort_index(), color='orange')\n",
        "    axs[0].set_title(title1)\n",
        "    axs[0].grid()\n",
        "\n",
        "    y = df_array2['emotion'].value_counts()\n",
        "    keys_missed = list(set(emotions.keys()).difference(set(y.keys())))\n",
        "    for key_missed in keys_missed:\n",
        "        y[key_missed] = 0\n",
        "    axs[1].bar(x, y.sort_index())\n",
        "    axs[1].set_title(title2)\n",
        "    axs[1].grid()\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSlXWS4sLfPW"
      },
      "outputs": [],
      "source": [
        "emotions = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DadLvnJDLfXH"
      },
      "outputs": [],
      "source": [
        "train_image_array, train_image_label = prepare_data(data2[data2[' Usage']=='Training'])\n",
        "val_image_array, val_image_label = prepare_data(data2[data2[' Usage']=='PrivateTest'])\n",
        "test_image_array, test_image_label = prepare_data(data2[data2[' Usage']=='PublicTest'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsvFtaqALfcW"
      },
      "outputs": [],
      "source": [
        "train_images = train_image_array.reshape((train_image_array.shape[0], 48, 48, 1))\n",
        "train_images = train_images.astype('float32')/255\n",
        "val_images = val_image_array.reshape((val_image_array.shape[0], 48,48,1))\n",
        "val_images = val_images.astype('float32')/255\n",
        "test_images = test_image_array.reshape((test_image_array.shape[0], 48, 48, 1))\n",
        "test_images = test_images.astype('float32')/255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKDNl9CeLfge"
      },
      "outputs": [],
      "source": [
        "train_labels = to_categorical(train_image_label)\n",
        "val_labels = to_categorical(val_image_label)\n",
        "test_labels = to_categorical(test_image_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adaY16ekLfkI"
      },
      "outputs": [],
      "source": [
        "plot_all_emotions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUsKBIPgLfoS"
      },
      "outputs": [],
      "source": [
        "plot_examples(label=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8o8pP2vWLfru"
      },
      "outputs": [],
      "source": [
        "plot_compare_distributions(train_labels, test_labels, title1='train labels', title2='test labels')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGjqR0WtLf5O"
      },
      "outputs": [],
      "source": [
        "class_weight = dict(zip(range(0, 7), (((data2[data2[' Usage']=='Training']['emotion'].value_counts()).sort_index())/len(data2[data2[' Usage']=='Training']['emotion'])).tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRAX_rwAL6iQ"
      },
      "outputs": [],
      "source": [
        "class_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GAMmXrvL6lv"
      },
      "outputs": [],
      "source": [
        "from keras import models\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dense\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(7, activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The convolutional layers are responsible for extracting features from input images, and the dense layers make predictions based on these features. Batch normalization helps stabilize and accelerate the training process. The softmax activation in the output layer is used for multi-class classification, providing probabilities for each class."
      ],
      "metadata": {
        "id": "7x9UmHAeFF4p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UW-EO87TL6pn"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Asj-hcTGL6tg"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model has a total of 1,422,087 parameters (weights and biases).\n",
        "1,421,191 parameters are trainable during the training process.\n",
        "896 parameters are non-trainable, likely related to batch normalization."
      ],
      "metadata": {
        "id": "k0AVJsWOF9nH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5faVScG2L6w5"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_images, train_labels,\n",
        "                    validation_data = (val_images, val_labels),\n",
        "                    class_weight = class_weight,\n",
        "                    epochs=10,\n",
        "                    batch_size=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " The loss values are decreasing, and accuracy is increasing, which is generally a positive sign, indicating that the model is learning from the data."
      ],
      "metadata": {
        "id": "qT_hAHGDHaPg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbY3FLbsL60W"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print('test accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Process:The trained model is tested on a set of data that it has not seen during training or validation. This set is called the test dataset.the model achieved an accuracy of approximately 56.1% on the test dataset, meaning it correctly predicted the labels for about 56.1% of the test samples."
      ],
      "metadata": {
        "id": "w1e15IH5H211"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MF6xkisSL63a"
      },
      "outputs": [],
      "source": [
        "pred_test_labels = model.predict(test_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XyhEUAhL66c"
      },
      "outputs": [],
      "source": [
        "pred_test_labels = np.argmax(pred_test_labels, axis=1)  # if test_labels contains probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKSsWZDdL69i"
      },
      "outputs": [],
      "source": [
        "pred_test_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOYILn6oMKHW"
      },
      "outputs": [],
      "source": [
        "test_labels = np.argmax(test_labels, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0NbrMVkMKL2"
      },
      "outputs": [],
      "source": [
        "test_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvaT79lOMKPx"
      },
      "outputs": [],
      "source": [
        "loss = history.history['loss']\n",
        "loss_val = history.history['val_loss']\n",
        "epochs = range(1, len(loss)+1)\n",
        "plt.plot(epochs, loss, 'bo', label='loss_train')\n",
        "plt.plot(epochs, loss_val, 'b', label='loss_val')\n",
        "plt.title('value of the loss function')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('value of the loss function')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-yq2kmSMKTj"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "acc_val = history.history['val_accuracy']\n",
        "epochs = range(1, len(loss)+1)\n",
        "plt.plot(epochs, acc, 'bo', label='accuracy_train')\n",
        "plt.plot(epochs, acc_val, 'b', label='accuracy_val')\n",
        "plt.title('accuracy')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('value of accuracy')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JECwq32_MKZT"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(test_labels, pred_test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OWVYmI6MUKQ"
      },
      "outputs": [],
      "source": [
        "cm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rows represent the actual (true) classes.\n",
        "Columns represent the predicted classes.\n",
        "                     Predicted\n",
        "                 |  0  |  1  |  2  |  3  |  4  |  5  |  6  |\n",
        "                 ------------------------------------------\n",
        "           True 0 | 210 |  3  |  62 |  31 |  89 |  19 |  53 |\n",
        "                 ------------------------------------------\n",
        "           True 1 | 105 | 182 |  35 |  14 |  42 |  0  |  14 |\n",
        "                 ------------------------------------------\n",
        "           True 2 | 55  |  2  | 191 |  28 | 107 |  54 |  59 |\n",
        "                 ------------------------------------------\n",
        "           True 3 | 43  |  2  |  46 | 610 |  75 |  37 |  82 |\n",
        "                 ------------------------------------------\n",
        "           True 4 | 69  |  4  |  88 |  39 | 315 |  31 | 107 |\n",
        "                 ------------------------------------------\n",
        "           True 5 | 17  |  0  |  47 |  20 |  11 | 308 |  12 |\n",
        "                 ------------------------------------------\n",
        "           True 6 | 57  |  1  |  66 |  54 | 118 |  25 | 286 |\n",
        "                 ------------------------------------------\n",
        "True Positives (TP): The diagonal elements (e.g., 210, 182, 191, 610, 315, 308, 286) represent the number of correct predictions for each class.\n",
        "False Positives (FP): The sum of values in each column (excluding the diagonal) gives the count of incorrect predictions for each predicted class.\n",
        "False Negatives (FN): The sum of values in each row (excluding the diagonal) gives the count of instances where the true class was not predicted.\n",
        "True Negatives (TN): The rest of the values in the matrix that are not on the diagonal, not in the row totals, and not in the column totals represent correctly predicted instances for the remaining classes."
      ],
      "metadata": {
        "id": "aHElumaDOxZK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZ-A5F6LMUOk"
      },
      "outputs": [],
      "source": [
        "alexnet = models.Sequential()\n",
        "\n",
        "# Layer 1\n",
        "alexnet.add(Conv2D(96, (11, 11), input_shape=(48, 48, 1), padding='same', activation='tanh'))\n",
        "alexnet.add(BatchNormalization())\n",
        "alexnet.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Layer 2\n",
        "alexnet.add(Conv2D(256, (5, 5), padding='same', activation='tanh'))\n",
        "alexnet.add(BatchNormalization())\n",
        "alexnet.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Layer 3\n",
        "alexnet.add(ZeroPadding2D((1, 1)))\n",
        "alexnet.add(Conv2D(512, (3, 3), padding='same', activation='tanh'))\n",
        "alexnet.add(BatchNormalization())\n",
        "alexnet.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Layer 4\n",
        "alexnet.add(ZeroPadding2D((1, 1)))\n",
        "alexnet.add(Conv2D(1024, (3, 3), padding='same', activation='tanh'))\n",
        "alexnet.add(BatchNormalization())\n",
        "\n",
        "# Layer 5\n",
        "alexnet.add(ZeroPadding2D((1, 1)))\n",
        "alexnet.add(Conv2D(1024, (3, 3), padding='same', activation='tanh'))\n",
        "alexnet.add(BatchNormalization())\n",
        "alexnet.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Layer 6\n",
        "alexnet.add(Flatten())\n",
        "alexnet.add(Dense(4096, activation='tanh'))\n",
        "alexnet.add(BatchNormalization())\n",
        "alexnet.add(Dropout(0.5))\n",
        "\n",
        "# Layer 7\n",
        "alexnet.add(Dense(4096, activation='tanh'))\n",
        "alexnet.add(BatchNormalization())\n",
        "alexnet.add(Dropout(0.5))\n",
        "\n",
        "# Layer 8\n",
        "alexnet.add(Dense(7, activation='softmax'))\n",
        "alexnet.add(BatchNormalization())\n",
        "\n",
        "# print model summary\n",
        "alexnet.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMHdf-zjMUSa"
      },
      "outputs": [],
      "source": [
        "alexnet.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36BLMzvBMUV3"
      },
      "outputs": [],
      "source": [
        "history = alexnet.fit(train_images, train_labels,\n",
        "                    validation_data = (val_images, val_labels),\n",
        "                    class_weight = class_weight,\n",
        "                    epochs=25,\n",
        "                    batch_size=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGg6nPz2McMI"
      },
      "outputs": [],
      "source": [
        " model=model.save('/content/drive/MyDrive/Mood-Detection-Project/challenges-in-representation-learning-facial-expression-recognition-challenge/model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "loaded_model = tf.keras.models.load_model('/content/drive/MyDrive/Mood-Detection-Project/challenges-in-representation-learning-facial-expression-recognition-challenge/model.h5')"
      ],
      "metadata": {
        "id": "Z5UDC1gmtvyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import PIL.Image\n",
        "from google.colab.patches import cv2_imshow\n",
        "import requests\n",
        "\n",
        "# Load pre-trained Haar Cascade classifier for face detection\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Load pre-trained machine learning model for mood prediction\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/Mood-Detection-Project/challenges-in-representation-learning-facial-expression-recognition-challenge/model.h5')\n",
        "\n",
        "# Define a dictionary to map predicted class indices to mood labels\n",
        "mood_labels = {0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy', 4: 'sad', 5: 'surprise', 6: 'neutral'}\n",
        "\n",
        "# Function to capture a photo using the webcam\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "    js = Javascript('''\n",
        "        async function takePhoto(quality) {\n",
        "            const div = document.createElement('div');\n",
        "            const capture = document.createElement('button');\n",
        "            capture.textContent = 'Capture';\n",
        "            div.appendChild(capture);\n",
        "\n",
        "            const video = document.createElement('video');\n",
        "            video.style.display = 'block';\n",
        "            const stream = await navigator.mediaDevices.getUserMedia({ 'video': true });\n",
        "\n",
        "            document.body.appendChild(div);\n",
        "            div.appendChild(video);\n",
        "            video.srcObject = stream;\n",
        "            await video.play();\n",
        "\n",
        "            // Resize the output to fit the video element.\n",
        "            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "            // Wait for the Capture to be clicked.\n",
        "            await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "            const canvas = document.createElement('canvas');\n",
        "            canvas.width = video.videoWidth;\n",
        "            canvas.height = video.videoHeight;\n",
        "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "            stream.getVideoTracks()[0].stop();\n",
        "            div.remove();\n",
        "            return canvas.toDataURL('image/jpeg', quality);\n",
        "        }\n",
        "    ''')\n",
        "    display(js)\n",
        "\n",
        "    data = eval_js('takePhoto({})'.format(quality))\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(binary)\n",
        "    return filename\n",
        "\n",
        "# Function to detect emotion from the image using a pre-trained emotion detection model\n",
        "def detect_emotion(image_path):\n",
        "     emotion = mood_label\n",
        "\n",
        "     return emotion\n",
        "\n",
        "# Function to search for songs on YouTube based on the detected emotion\n",
        "def search_songs(emotion):\n",
        "    query = f'{emotion} songs'\n",
        "\n",
        "    # URL encode the query\n",
        "    query = requests.utils.quote(query, safe='')\n",
        "\n",
        "    # YouTube search URL\n",
        "    youtube_search_url = f'https://www.youtube.com/results?search_query={query}'\n",
        "\n",
        "    # Open the YouTube search URL\n",
        "    display(Javascript(f'window.open(\"{youtube_search_url}\",\"_blank\");'))\n",
        "\n",
        "# Loop over frames from the webcam\n",
        "while True:\n",
        "    # Capture a photo using the webcam\n",
        "    image_path = take_photo()\n",
        "\n",
        "    # Read the captured image\n",
        "    frame = cv2.imread(image_path)\n",
        "\n",
        "    # Convert frame to grayscale\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces in frame using Haar Cascade classifier\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
        "\n",
        "    # Loop over detected faces\n",
        "    for (x, y, w, h) in faces:\n",
        "        # Extract face ROI (Region of Interest)\n",
        "        face_roi = gray[y:y+h, x:x+w]\n",
        "        # Resize face ROI to fit model input size\n",
        "        face_roi = cv2.resize(face_roi, (48, 48))\n",
        "        # Normalize pixel values to range [0, 1]\n",
        "        face_roi = face_roi / 255.0\n",
        "        # Reshape face ROI to match model input shape\n",
        "        face_roi = np.reshape(face_roi, (1, 48, 48, 1))\n",
        "        # Make mood prediction using pre-trained model\n",
        "        prediction = model.predict(face_roi)\n",
        "        # Get predicted mood label\n",
        "        mood_label = mood_labels[np.argmax(prediction)]\n",
        "\n",
        "        # Detect emotion from the image\n",
        "        emotion = detect_emotion(image_path)\n",
        "\n",
        "        # Draw bounding box around detected face\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "        # Write predicted mood label and detected emotion on top of bounding box\n",
        "        text = f'{mood_label}, Emotion: {emotion}'\n",
        "        cv2.putText(frame, text, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "        # Search for songs on YouTube based on the detected emotion\n",
        "        search_songs(emotion)\n",
        "\n",
        "    # Display output frame\n",
        "    cv2_imshow(frame)\n",
        "\n",
        "    # Ask the user if they want to continue or exit\n",
        "    user_input = input(\"Want to continue? Enter 'c' to continue, 'q' to exit: \")\n",
        "    if user_input.lower() == 'q':\n",
        "        break\n",
        "\n",
        "# Close all windows\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "CwMxaLdFwucE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}